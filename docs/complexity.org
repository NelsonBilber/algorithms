#+TITLE: Algorithms Complexity
#+AUTHOR: Nelson Bilber Rodrigues


/Asymptotic Analysis is the big idea that handles above issues in analyzing algorithms. In Asymptotic Analysis, we evaluate the performance of an algorithm in terms of input size (we donâ€™t measure the actual running time). We calculate, how does the time (or space) taken by an algorithm increases with the input size./

from: [[https://www.geeksforgeeks.org/analysis-of-algorithms-set-1-asymptotic-analysis/][Analysis of Algorithms | Set 1 (Asymptotic Analysis)]]

* Big-Oh
Big-oh notation relates the number of operations needed to process a number of inputs. If input increases it can measures how long current algorithm will need to process all input.

Use cases:

- Measure the worst possible case for search and item on a list
- How algorithm will behave when sorting a list


[[file:/docs/imgs/complexity.png]]

Number of operations N versus input size n for each function

credits image: [[https://en.wikipedia.org/wiki/Big_O_notation#/media/File:Comparison_computational_complexity.svg][Big-Oh graph]] 

*** How to calculate it ?
**** Constant

O(1) - constant rate 
growing rate = 1

#+BEGIN_SRC C++
i++
#+END_SRC

**** Logarithmic

O(log(n)) - divide in half, eg. typical binary search
growing rate = log(n)

#+BEGIN_SRC C++
for(int i = 0;i< 1000;i++)
{
	i = i/2;
}
#+END_SRC

**** Linear

growing rate: n

#+BEGIN_SRC C++
for (int i = 0; i < n; i++) 
{
	r += 1;
}
#+END_SRC
The loop executes N times, so the sequence of statements also executes N times. If we assume the statements are O(1), the total time for the for loop is N * O(1), which is O(N) overall.
**** Quadratic
growing rate: n*log(n)
#+BEGIN_SRC C++
//Double loop(nested loops)
for (int c = 0; c < n; c++) 
{
	for (int i = 0; i < n; i++) 
	{
		// sequence of statements
		a += 1;
  	}
}
#+END_SRC

Growth Rate: n^2

" /The outer loop executes N times. Every time the outer loop executes, the inner loop executes M times. As a result, the statements in the inner loop execute a total of N * M times. Thus, the complexity is O(N * M). In a common special case where the stopping condition of the inner loop is J < N instead of J < M (i.e., the inner loop also executes N times), the total complexity for the two loops is O(N2)./ "
 
from: [[https://github.com/donbeave/interview][Data Structures and Algorithms in Java]]

e.g. Mergesort, Quicksort

**** Exponential 

e.g. Generate all possible solutions to find a value, example a key.

**** Code that calls other functions, how to calculate it ?
 
" /When a statement involves a function call, the complexity of the statement includes the complexity of the function. Assume that you know that function f takes constant time, and that function g takes time proportional to (linear in) the value of its parameter k. Then the statements below have the time complexities indicated./ "

f(k) has O(1) g(k) has O(k)

from: [[https://github.com/donbeave/interview][Data Structures and Algorithms in Java]]


* Big Theta
	
* Big Omega

* Links 

[[https://github.com/donbeave/interview][Data Structures and Algorithms in Java]]

[[https://justin.abrah.ms/computer-science/understanding-big-o-formal-definition.html][Understanding the formal definition of Big-O]]
